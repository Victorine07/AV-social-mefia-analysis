{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f5f97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import re\n",
    "import string\n",
    "from itertools import groupby\n",
    "import io\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a1aa69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Combined US\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "450b09b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       connected and automated vehicles and new tech...\n",
      "1       examples of mdot efforts in this field , whic...\n",
      "2       implementation and test facilities through pl...\n",
      "3       new technology e-construction: mdot’s e -cons...\n",
      "4       emergency response, coordinate information fo...\n",
      "                             ...                        \n",
      "707      carmasm and cooperative driving automation c...\n",
      "708     virtual open innovation collaborative environ...\n",
      "709                                        what’s next? \n",
      "710     what’s next? automated driving systems have t...\n",
      "711                   www.transportation.gov/av january \n",
      "Name: full text, Length: 712, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# MAKING EVERYTHING LOWERCASE\n",
    "\n",
    "df = pd.read_csv(filename + \".csv\", encoding='utf-8-sig')\n",
    "df['full text'] = df['full text'].str.lower()\n",
    "df.to_csv(filename + \" cleaned.csv\", index = False, encoding='utf-8-sig')\n",
    "print(df['full text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9feac7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename + \" cleaned.csv\", encoding='utf-8-sig')\n",
    "df = df.drop(df.columns[[1]], axis=1, inplace = False)\n",
    "#df = df.drop(df.columns[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]], axis=1, inplace = False)\n",
    "df.to_csv(filename + \" cleaned.csv\", index = False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "550e758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      connected and automated vehicles and new techn...\n",
      "1      examples of mdot efforts in this field , which...\n",
      "2      implementation and test facilities through pla...\n",
      "3      new technology e-construction: mdot’s e -const...\n",
      "4      emergency response, coordinate information for...\n",
      "                             ...                        \n",
      "707    carmasm and cooperative driving automation car...\n",
      "708    virtual open innovation collaborative environm...\n",
      "709                                         what’s next?\n",
      "710    what’s next? automated driving systems have th...\n",
      "711                                              january\n",
      "Name: full text, Length: 712, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# DELETING MENTIONS, HASHTAGS, AND LINKS\n",
    "\n",
    "df = pd.read_csv(filename + \" cleaned.csv\", encoding='utf-8-sig')\n",
    "\n",
    "def remove_words(in_list, char_list):\n",
    "    new_list = []\n",
    "    for line in in_list:\n",
    "        new_words = ' '.join([word for word in line.split() if not any([phrase in word for phrase in char_list])])\n",
    "        new_list.append(new_words)\n",
    "    return new_list\n",
    "     \n",
    "str_list = df['full text']\n",
    "char_list = ['www', 'https']\n",
    "df['full text'] = remove_words(str_list, char_list)\n",
    "df.to_csv(filename + \" cleaned.csv\", index = False, encoding='utf-8-sig')\n",
    "print(df['full text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "758e5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETING WHITESPACE\n",
    "\n",
    "df = pd.read_csv(filename + \" cleaned.csv\", encoding='utf-8-sig', skipinitialspace = True)\n",
    "df = df.replace(r'\\n',' ', regex=True)\n",
    "df['full text'] = df['full text'].replace(r'\\s+', ' ', regex=True)\n",
    "df.dropna(subset = ['full text'], inplace=True)\n",
    "df.to_csv(filename + \" cleaned.csv\", index = False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf7cf90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'im', 'u', '&', 'amp', 'us', 'may', 'thing', 'isnt', 'dont', \"i'm\", 'i’m', \"we've\", 's', 't', 'c', 'also', 'page', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'et', 'al', 'uni']\n"
     ]
    }
   ],
   "source": [
    "# DEFINES THE STOPWORDS\n",
    "\n",
    "new_stopwords = [\"im\", \"u\", \"&\", 'amp', 'us', 'may', 'thing', 'isnt', 'dont', \"i'm\", \"i’m\", \"we've\", 's', 't', 'c', 'also', 'page', 'a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
    "'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'et', 'al', 'uni']                 \n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.extend(new_stopwords)\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f573fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      connected automated vehicles new technology wh...\n",
      "1      examples mdot efforts field , involve infrastr...\n",
      "2      implementation test facilities planet planet r...\n",
      "3      new technology e-construction: mdot’s -constru...\n",
      "4      emergency response, coordinate information pla...\n",
      "                             ...                        \n",
      "707    carmasm cooperative driving automation carma f...\n",
      "708    virtual open innovation collaborative environm...\n",
      "709                                         what’s next?\n",
      "710    what’s next? automated driving systems potenti...\n",
      "711                                              january\n",
      "Name: full text, Length: 712, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# REMOVING STOPWORDS\n",
    "\n",
    "df = pd.read_csv(filename + \" cleaned.csv\", encoding='utf-8-sig')\n",
    "pos_tweets = df['full text']\n",
    "\n",
    "test = pd.DataFrame(pos_tweets)\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "df['full text'] = df['full text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "#df['tweet'] = test['tweet_without_stopwords']\n",
    "print(df['full text'])\n",
    "df.to_csv(filename + \" cleaned.csv\", index = False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e4e0aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessy\\AppData\\Local\\Temp/ipykernel_34456/2008078429.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['full text'] = df['full text'].str.replace('[^\\w\\s]',' ')\n"
     ]
    }
   ],
   "source": [
    "# REMOVES PUNCTUATION\n",
    "\n",
    "df = pd.read_csv(filename + \" cleaned.csv\", encoding='utf-8-sig')\n",
    "df['full text'] = df['full text'].str.replace('[^\\w\\s]',' ')\n",
    "#df['hashtags'] = df['hashtags'].str.replace('[^\\w\\s]','')\n",
    "df.to_csv(filename + \" cleaned.csv\", index = False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e8df927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETING WHITESPACE #2\n",
    "\n",
    "df = pd.read_csv(filename + \" cleaned.csv\", encoding='utf-8-sig', skipinitialspace = True)\n",
    "df = df.replace(r'\\n',' ', regex=True)\n",
    "df['full text'] = df['full text'].replace(r'\\s+', ' ', regex=True)\n",
    "df.dropna(subset = ['full text'], inplace=True)\n",
    "df.to_csv(filename + \" cleaned.csv\", index = False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a458efd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessy\\AppData\\Local\\Temp/ipykernel_34456/1682947.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['full text'] = df['full text'].str.replace('\\d+', '')\n"
     ]
    }
   ],
   "source": [
    "# DELETING NUMBERS\n",
    "\n",
    "df = pd.read_csv(filename + \" cleaned.csv\", encoding='utf-8-sig')\n",
    "df['full text'] = df['full text'].str.replace('\\d+', '')\n",
    "df.to_csv(filename + \" cleaned.csv\", index = False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a023660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DELETING OTHER DUPE TWEETS\n",
    "\n",
    "# df = pd.read_csv(filename + \" cleaned.csv\", encoding='utf-8-sig')\n",
    "# df = df[df[\"tweet\"].str.contains(\"poltergeist\") == False]\n",
    "# df = df[df[\"tweet\"].str.contains(\"cliche things unavoidable life death taxes probably add new technology list artificial intelligence\") == False]\n",
    "# df = df[df[\"tweet\"].str.contains(\"environmental tradeoffs autonomous vehicles convenience likely come\") == False]\n",
    "# df = df[df[\"tweet\"].str.contains(\"tory mps squealing big brother cockpit putting risk lives british roads also undermining uk leadership promoting future deployment connected autonomous vehicles cavs\") == False]\n",
    "# #df = df[df[\"tweet\"].str.contains(\"concerns about safe deployment of autonomous vehicles aired at congressional hearing\") == False]\n",
    "# #df = df[df[\"tweet\"].str.contains(\"to make the most of autonomous vehicles advantages and avoid the disadvantages we must choose to shape our cities\") == False]\n",
    "# #df = df[df[\"tweet\"].str.contains(\"autonomous vehicles hold great promise to deliver significant benefits for all americans  but only if the federal government puts the necessary policies in place to achieve these benefits\") == False]\n",
    "# #df = df[df[\"tweet\"].str.contains(\"semiautonomous vehicles are creating an injury litigation risk for insurers\") == False]\n",
    "# #df = df[df[\"tweet\"].str.contains(\"the cliche is that the only things unavoidable in life are death and taxes we can probably add new technology to the list too artificial intelligence\") == False]\n",
    "# #df = df[df[\"tweet\"].str.contains(\"tory mps squealing about big brother in the cockpit are not just putting at risk lives on british roads but are also undermining uk leadership in promoting the future deployment of connected and autonomous vehicles cavs\") == False]\n",
    "# #df = df[df[\"tweet\"].str.contains(\"3 security issues facing selfdriving cars\") == False]\n",
    "# #df = df[df[\"tweet\"].str.contains(\"poltergeist\") == False]\n",
    "# #df = df[df[\"tweet\"].str.contains(\"poltergeist\") == False]\n",
    "# #df = df[df[\"tweet\"].str.contains(\"poltergeist\") == False]\n",
    "# #df = df[df[\"tweet\"].str.contains(\"poltergeist\") == False]\n",
    "# sizeafter = df.shape\n",
    "# print('size before =', sizeinitial)\n",
    "# print('size after =', sizeafter)\n",
    "# df.to_csv(filename + \" cleaned.csv\", index = False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b7f9fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DELETING DUPLICATE TWEETS\n",
    "\n",
    "# df = pd.read_csv(filename + \" cleaned.csv\", encoding='utf-8-sig')\n",
    "# df = df.drop_duplicates(subset=['tweet'], inplace=False)\n",
    "# sizeafter = df.shape\n",
    "# print('size before =', sizeinitial)\n",
    "# print('size after =', sizeafter)\n",
    "# df.to_csv(filename + \" cleaned.csv\", index = False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "42cd894d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      connected automated vehicles new technology wh...\n",
      "1      examples mdot efforts field involve infrastruc...\n",
      "2      implementation test facilities planet planet r...\n",
      "3      new technology construction mdot construction ...\n",
      "4      emergency response coordinate information plan...\n",
      "                             ...                        \n",
      "706    carmasm cooperative driving automation carma f...\n",
      "707    virtual open innovation collaborative environm...\n",
      "708                                                 next\n",
      "709    next automated driving systems potential signi...\n",
      "710                                              january\n",
      "Name: full text, Length: 711, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# REMOVING STOPWORDS #2\n",
    "\n",
    "df = pd.read_csv(filename + \" cleaned.csv\", encoding='utf-8-sig')\n",
    "pos_tweets = df['full text']\n",
    "\n",
    "test = pd.DataFrame(pos_tweets)\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "df['full text'] = df['full text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "#df['tweet'] = test['tweet_without_stopwords']\n",
    "print(df['full text'])\n",
    "df.to_csv(filename + \" cleaned.csv\", index = False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11bea0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETING WHITESPACE #3\n",
    "\n",
    "df = pd.read_csv(filename + \" cleaned.csv\", encoding='utf-8-sig', skipinitialspace = True)\n",
    "df = df.replace(r'\\n',' ', regex=True)\n",
    "df['full text'] = df['full text'].replace(r'\\s+', ' ', regex=True)\n",
    "df.dropna(subset = ['full text'], inplace=True)\n",
    "df.to_csv(filename + \" cleaned.csv\", index = False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b78ab13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ONLY TWEETS\n",
    "\n",
    "# df = pd.read_csv(filename + \" cleaned.csv\", encoding='utf-8-sig')\n",
    "# df = df.drop(df.columns[[0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11]], axis=1, inplace = False)\n",
    "# df.to_csv(filename + \" only tweets cleaned.csv\", index = False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab478e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d010e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
