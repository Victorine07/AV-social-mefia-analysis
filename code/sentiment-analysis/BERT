The BERT model was proposed in BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. Itâ€™s a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.

The source code used for sentiment analysis can be found: 
https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face

the labeled dataset for twitter sentiment analysis on AVs was found:
https://github.com/ClimbsRocks/nlpSentiment/blob/master/twitterCorpus/Twitter-sentiment-self-drive-DFE.csv



