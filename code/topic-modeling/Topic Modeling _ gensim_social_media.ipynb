{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3ddbda9",
   "metadata": {},
   "source": [
    "#### Topic Modeling ___ Gensim\n",
    "\n",
    "Here, we will do topic modeling using LDA from gensim library and see the diff with LDA in sklearn \n",
    "\n",
    "\n",
    "We will do cybersecurity category only for this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad0be5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d57f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer2 = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "#from sklearn.utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c5216e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim import\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# from gensim.utils import simple_preprocess\n",
    "# from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9af0954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create our own stopwords library, removing the most frequent AV terms as well\n",
    "stopwords_AV = [\n",
    "                # we removed all the quastionning indicators (who, when...) and negation from stopwords\n",
    "                'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", \n",
    "                'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',\n",
    "                'themselves',  'this', 'that', \"that'll\", 'these', 'those', 'am', \n",
    "                'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "                'did', 'doing', 'a', 'an', 'the', 'and', 'or', 'of',\n",
    "                'at', 'by', 'for', 'with',  'into', 'through', 'during', \n",
    "                'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', \n",
    "                 'here', 'there', 'all', 'any', \n",
    "                'both' , 'more', 'other', 'such', 'own', \n",
    "                'same', 'so', 'than', 'too', 'very', 's', 't', 'just', \n",
    "                 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',  'ma',\n",
    "                # context words -- will be kept for more context\n",
    "                # 'driving', 'self', 'vehicles', 'autonomous', 'car', 'driverless'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7686a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning functions\n",
    "\n",
    "# WORDNET LEMMATIZER (with appropriate pos tags)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define function to lemmatize each word with its POS tag\n",
    "\n",
    "# POS_TAGGER_FUNCTION : TYPE 1\n",
    "def pos_tagger(nltk_tag):\n",
    "\tif nltk_tag.startswith('J'):\n",
    "\t\treturn wordnet.ADJ\n",
    "\telif nltk_tag.startswith('V'):\n",
    "\t\treturn wordnet.VERB\n",
    "\telif nltk_tag.startswith('N'):\n",
    "\t\treturn wordnet.NOUN\n",
    "\telif nltk_tag.startswith('R'):\n",
    "\t\treturn wordnet.ADV\n",
    "\telse:\t\t\n",
    "\t\treturn None\n",
    "\n",
    "def lemmatization(sentence):\n",
    "    # tokenize the sentence and find the POS tag for each token\n",
    "    pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "\n",
    "    # we use our own pos_tagger function to make things simpler to understand.\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            # if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:\t\n",
    "            # else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "\n",
    "    return lemmatized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f590e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cleaning the Data \n",
    "\n",
    "# 1. removing twitter handkes\n",
    "\n",
    "\n",
    "def remove_pattern(input_text, pattern):\n",
    "    r=re.findall(pattern,input_text)\n",
    "    for i in r:\n",
    "        input_text=re.sub(i,'',input_text)\n",
    "    return input_text\n",
    "\n",
    "def cleaning_data(df2, column='tweet'):\n",
    "    df2 =df2.drop_duplicates()\n",
    "    \n",
    "    # 2.removing tweeter handles @user\n",
    "    df2['clean_twt'] = np.vectorize(remove_pattern)(df2[column],'@[\\w]*')\n",
    "    df2['clean_twt'] = np.vectorize(remove_pattern)(df2['clean_twt'],'#[\\w]*')\n",
    "    #df2['clean_twt'] = df2['clean_twt'].apply (lambda x :  ' '.join(re.sub(\"@[A-Za-z0-9_]+\",\"\", x).split())\n",
    "    df2['clean_twt'] = df2['clean_twt'].str.replace('http\\S+|www\\S+','', case=False)\n",
    "\n",
    "    ##spacy cleaning_ contractions removal\n",
    "    df2.loc[:15,'clean_twt'] = df2.loc[:15,'clean_twt'].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x)])) \n",
    "    df2['clean_twt'] = df2['clean_twt'].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x)])) \n",
    "    \n",
    "    \n",
    "    # 3.rm punctuation, numbers, special char\n",
    "    df2['clean_twt'] = df2['clean_twt'].str.replace('[^A-Za-z0-9]+',' ')\n",
    "    df2['clean_twt'] = df2['clean_twt'].apply (lambda x : ' '.join([item for item in str(x).split() if not item.isdigit()]))\n",
    "    # words with digirs\n",
    "    df2['clean_twt'] = df2['clean_twt'].apply (lambda x : ' '.join(s for s in x.split() if not any(c.isdigit() for c in s)))\n",
    "\n",
    "    # 5. lowercase\n",
    "    df2['clean_twt'] = df2['clean_twt'].apply(lambda x: ''.join([w for w in x.lower()]))\n",
    "\n",
    "    #lemmatization with context\n",
    "    df2['clean_twt'] = df2['clean_twt'].apply(lambda x: ' '.join([ lemmatization(item) for item in str(x).split() if item not in stopwords_AV]))\n",
    "    \n",
    "    #spacy cleaning_ contractions removal\n",
    "    df2['clean_twt'] = df2['clean_twt'].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x)])) \n",
    "    \n",
    "    #stop = stopwords.words('english')\n",
    "   # df2['clean_twt'] = df2['clean_twt'].apply(lambda x: ' '.join([item for item in str(x).split() if item not in stopwords_AV ]))\n",
    "    \n",
    "    # let remove all stopwords; they weight in the topic modeling\n",
    "    stop = stopwords.words('english')\n",
    "    df2['clean_twt'] = df2['clean_twt'].apply(lambda x: ' '.join([item for item in str(x).split() if item not in stop]))\n",
    "    \n",
    "    # for more context understanding and exploitation, we want to change all synonyms of AVs like driverless car/vehicle,\n",
    "    # self driving car/vehicle; self drive vehicle/car, autonomous car by 'autinimous vehicle)\n",
    "\n",
    "    elts_replace = ['driverless car', 'driverless vehicle', 'self driving car','self driving vehicle', \n",
    "                'self drive vehicle', 'self drive car', 'autonomous car' , 'drive self car']\n",
    "    # replacing those words with 'autonomous vehicles'\n",
    "    df2['clean_twt_re'] = df2['clean_twt']\n",
    "\n",
    "    \n",
    "    for item in elts_replace:\n",
    "        df2['clean_twt_re'] = df2['clean_twt_re'].str.replace(item,'automate vehicle')\n",
    "    \n",
    "    df2['clean_twt_re'] = df2['clean_twt_re'].str.replace('car','vehicle')\n",
    "    df2['clean_twt_re'] = df2['clean_twt_re'].str.replace('self drive','automate')\n",
    "    df2['clean_twt_re'] = df2['clean_twt_re'].str.replace('selfdrive','automate')\n",
    "    df2['clean_twt_re'] = df2['clean_twt_re'].str.replace('selfdriving','automate')\n",
    "    df2['clean_twt_re'] = df2['clean_twt_re'].str.replace('driverless','automate')\n",
    "    df2['clean_twt_re'] = df2['clean_twt_re'].str.replace('automated','automate')\n",
    "    df2['clean_twt_re'] = df2['clean_twt_re'].str.replace('autonomous','automate')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16023f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other preprocessing process\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['come','try','go','get','make','would','really','like','great','came','got','rt']) \n",
    "\n",
    "def strip_newline(series):\n",
    "    return [review.replace('\\n','') for review in series]\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "# def remove_stopwords(texts):\n",
    "#     return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    out = [[word for word in simple_preprocess(str(doc))\n",
    "            if word not in stop_words]\n",
    "            for doc in texts]\n",
    "    return out\n",
    "\n",
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod\n",
    "\n",
    "def get_corpus(df, column):\n",
    "    df[column] = strip_newline(df[column])\n",
    "    words = list(sent_to_words(df[column]))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram_mod = bigrams(words)\n",
    "    bigram = [bigram_mod[review] for review in words]\n",
    "    id2word = gensim.corpora.Dictionary(bigram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "    \n",
    "    return corpus, id2word, bigram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c7c723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae31e9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71d119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d603cbd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4b56b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading the data\n",
    "# getting the data and cleaning it \n",
    "\n",
    "df_sec_twit = pd.read_csv(\"Categorized Data/Twitter/accidents.csv\", encoding_errors='ignore')\n",
    "df_sec_redt = pd.read_csv(\"Categorized Data/Reddit/Cybersecurity_Reddit.csv\", encoding_errors='ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "af348e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(730, 1)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sec_twit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9b54342a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt'] = np.vectorize(remove_pattern)(df2[column],'@[\\w]*')\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt'] = np.vectorize(remove_pattern)(df2['clean_twt'],'#[\\w]*')\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df2['clean_twt'] = df2['clean_twt'].str.replace('http\\S+|www\\S+','', case=False)\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt'] = df2['clean_twt'].str.replace('http\\S+|www\\S+','', case=False)\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt'] = df2['clean_twt'].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x)]))\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:27: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df2['clean_twt'] = df2['clean_twt'].str.replace('[^A-Za-z0-9]+',' ')\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt'] = df2['clean_twt'].str.replace('[^A-Za-z0-9]+',' ')\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt'] = df2['clean_twt'].apply (lambda x : ' '.join([item for item in str(x).split() if not item.isdigit()]))\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt'] = df2['clean_twt'].apply (lambda x : ' '.join(s for s in x.split() if not any(c.isdigit() for c in s)))\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt'] = df2['clean_twt'].apply(lambda x: ''.join([w for w in x.lower()]))\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt'] = df2['clean_twt'].apply(lambda x: ' '.join([ lemmatization(item) for item in str(x).split() if item not in stopwords_AV]))\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt'] = df2['clean_twt'].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x)]))\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt'] = df2['clean_twt'].apply(lambda x: ' '.join([item for item in str(x).split() if item not in stop]))\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt_re'] = df2['clean_twt']\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt_re'] = df2['clean_twt_re'].str.replace(item,'automate vehicle')\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt_re'] = df2['clean_twt_re'].str.replace('car','vehicle')\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt_re'] = df2['clean_twt_re'].str.replace('self drive','automate')\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt_re'] = df2['clean_twt_re'].str.replace('selfdrive','automate')\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt_re'] = df2['clean_twt_re'].str.replace('selfdriving','automate')\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt_re'] = df2['clean_twt_re'].str.replace('driverless','automate')\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt_re'] = df2['clean_twt_re'].str.replace('automated','automate')\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['clean_twt_re'] = df2['clean_twt_re'].str.replace('autonomous','automate')\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df2['clean_twt'] = df2['clean_twt'].str.replace('http\\S+|www\\S+','', case=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/2152554357.py:27: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df2['clean_twt'] = df2['clean_twt'].str.replace('[^A-Za-z0-9]+',' ')\n"
     ]
    }
   ],
   "source": [
    "# cleaning the tweets and comments\n",
    "df_sec_twit = cleaning_data(df_sec_twit,'tweet')\n",
    "df_sec_redt = cleaning_data(df_sec_redt,'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52acc171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "57860b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/1534098437.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = strip_newline(df[column])\n"
     ]
    }
   ],
   "source": [
    "train_corpus4_twit, train_id2word_twit, bigram_train_twit = get_corpus(df_sec_twit,'clean_twt_re')\n",
    "train_corpus4_redt, train_id2word_redt, bigram_train_redt = get_corpus(df_sec_redt,'clean_twt_re')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd293d",
   "metadata": {},
   "source": [
    "#####  chosing the number of topics for LDA\n",
    "Create a handful of LDA models with different topic values, then see how these perform in the supervised classification model training. This is specific to my goals here, since my ultimate aim is to see if the topic distributions have predictive value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d865e39",
   "metadata": {},
   "source": [
    "### LDA Model in Gensim\n",
    "Note that running eval_every=1 does this in batches of chunksize batches. The 20 is chosen from a HDP Process Test on the same data in another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9998849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    lda_train4 = gensim.models.ldamulticore.LdaMulticore(\n",
    "                           corpus=train_corpus4_twit,\n",
    "                           num_topics=5,\n",
    "                           id2word=train_id2word_twit,\n",
    "                           random_state=100,\n",
    "                           chunksize=10,\n",
    "                           workers=7, # Num. Processing Cores - 1\n",
    "                           passes=10,\n",
    "                           eval_every = 1,\n",
    "                           iterations=100,\n",
    "                           per_word_topics=True)\n",
    "    lda_train4.save('lda_train4.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6cce8862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.075*\"system\" + 0.060*\"link\" + 0.052*\"time\" + 0.049*\"safety\" + 0.046*\"accident\" + 0.042*\"via\" + 0.040*\"liability\" + 0.039*\"could\" + 0.033*\"tech\" + 0.030*\"collision\" + 0.027*\"ai\" + 0.024*\"still\" + 0.022*\"less\" + 0.022*\"new\" + 0.021*\"law\"'),\n",
       " (1,\n",
       "  '0.094*\"drive\" + 0.054*\"accident\" + 0.036*\"human\" + 0.036*\"driver\" + 0.032*\"safe\" + 0.027*\"safety\" + 0.025*\"control\" + 0.024*\"new_york\" + 0.023*\"need\" + 0.022*\"fully\" + 0.022*\"cause\" + 0.022*\"auto\" + 0.021*\"many\" + 0.021*\"elon_musk\" + 0.020*\"regulator\"'),\n",
       " (2,\n",
       "  '0.101*\"accident\" + 0.069*\"road\" + 0.065*\"driver\" + 0.057*\"tesla\" + 0.045*\"say\" + 0.038*\"test\" + 0.037*\"software\" + 0.033*\"need\" + 0.032*\"well\" + 0.027*\"responsible\" + 0.024*\"look\" + 0.023*\"problem\" + 0.022*\"lead\" + 0.022*\"day\" + 0.021*\"maybe\"'),\n",
       " (3,\n",
       "  '0.098*\"tesla\" + 0.060*\"report\" + 0.056*\"accident\" + 0.052*\"involve\" + 0.036*\"nearly\" + 0.033*\"use\" + 0.033*\"full\" + 0.032*\"cause\" + 0.032*\"know\" + 0.026*\"datum\" + 0.023*\"much\" + 0.019*\"injury\" + 0.019*\"release\" + 0.017*\"year\" + 0.017*\"time\"'),\n",
       " (4,\n",
       "  '0.085*\"driverassist_technology\" + 0.081*\"link_hundred\" + 0.044*\"datum_show\" + 0.043*\"tesla\" + 0.037*\"national_highway\" + 0.034*\"traffic_safety\" + 0.034*\"autopilot\" + 0.032*\"month\" + 0.032*\"administration\" + 0.027*\"nhtsa\" + 0.025*\"involve\" + 0.023*\"report\" + 0.023*\"want\" + 0.021*\"drive\" + 0.021*\"release\"')]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lda_train4.print_topics(10,num_words=15)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2f963f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sec_twit.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "43fe0787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/573457323.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
      "/var/folders/43/x1c5gssn601526kgzp78x6dm0000gn/T/ipykernel_4952/573457323.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Document_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8992</td>\n",
       "      <td>drive, accident, human, driver, safe, safety, control, new_york, need, fully</td>\n",
       "      <td>[automate, vehicle, order, magnitude, safe, prevent, many, type, accident, human, driver, error]</td>\n",
       "      <td>automate vehicle order magnitude safe prevent many type accident human driver error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8659</td>\n",
       "      <td>drive, accident, human, driver, safe, safety, control, new_york, need, fully</td>\n",
       "      <td>[solution, allow, automate, vehicle, put, insurance, brand, plan, insurance, plan, cover, financial, cost, accident, insure, vehicle, exclusive, automate, vehicle, brand, insurer, must, exist]</td>\n",
       "      <td>solution allow automate vehicle put insurance brand plan insurance plan cover financial cost accident insure vehicle exclusive automate vehicle brand insurer must exist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7980</td>\n",
       "      <td>accident, road, driver, tesla, say, test, software, need, well, responsible</td>\n",
       "      <td>[add, benefit, test, automate, vehicle, virtual, world, first, minimise, chance, real, accident]</td>\n",
       "      <td>add benefit test automate vehicle virtual world first minimise chance real accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5205</td>\n",
       "      <td>drive, accident, human, driver, safe, safety, control, new_york, need, fully</td>\n",
       "      <td>[fully, automate, vehicle, nearly, impossible, totally, safe, due, human, factor, automate, vehicle, road, interconnect, number, vehicle, accident, could, drop, nearly, zero, need, remove, human, error, human]</td>\n",
       "      <td>fully automate vehicle nearly impossible make totally safe due human factor automate vehicle road interconnect number vehicle accident could drop nearly zero need remove human error human unpredictability</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Document_Topic  Topic_Perc_Contrib  \\\n",
       "0            0               1              0.8992   \n",
       "1            1               1              0.8659   \n",
       "2            2               2              0.7980   \n",
       "3            3               1              0.5205   \n",
       "\n",
       "                                                                       Keywords  \\\n",
       "0  drive, accident, human, driver, safe, safety, control, new_york, need, fully   \n",
       "1  drive, accident, human, driver, safe, safety, control, new_york, need, fully   \n",
       "2   accident, road, driver, tesla, say, test, software, need, well, responsible   \n",
       "3  drive, accident, human, driver, safe, safety, control, new_york, need, fully   \n",
       "\n",
       "                                                                                                                                                                                                                Text  \\\n",
       "0                                                                                                                   [automate, vehicle, order, magnitude, safe, prevent, many, type, accident, human, driver, error]   \n",
       "1                   [solution, allow, automate, vehicle, put, insurance, brand, plan, insurance, plan, cover, financial, cost, accident, insure, vehicle, exclusive, automate, vehicle, brand, insurer, must, exist]   \n",
       "2                                                                                                                   [add, benefit, test, automate, vehicle, virtual, world, first, minimise, chance, real, accident]   \n",
       "3  [fully, automate, vehicle, nearly, impossible, totally, safe, due, human, factor, automate, vehicle, road, interconnect, number, vehicle, accident, could, drop, nearly, zero, need, remove, human, error, human]   \n",
       "\n",
       "                                                                                                                                                                                                   Cleaned_Text  \n",
       "0                                                                                                                           automate vehicle order magnitude safe prevent many type accident human driver error  \n",
       "1                                      solution allow automate vehicle put insurance brand plan insurance plan cover financial cost accident insure vehicle exclusive automate vehicle brand insurer must exist  \n",
       "2                                                                                                                           add benefit test automate vehicle virtual world first minimise chance real accident  \n",
       "3  fully automate vehicle nearly impossible make totally safe due human factor automate vehicle road interconnect number vehicle accident could drop nearly zero need remove human error human unpredictability  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=train_corpus4_redt, texts=bigram_train_redt, cleaned_text=bigram_train_redt):\n",
    "    #init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
    "        #print (row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        #get the dominant topic, Perc Contribubion and keywords for each document\n",
    "        for j, (topic_num,prop_topic) in enumerate(row):\n",
    "            if j==0: # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    \n",
    "    # add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    # cleaned_text = \n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents, cleaned_text], axis =1)\n",
    "    return sent_topics_df\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel= lda_train4, corpus=train_corpus4_twit , texts=bigram_train_twit, cleaned_text=df_sec_twit.clean_twt_re  )\n",
    "\n",
    "# format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Document_Topic', 'Topic_Perc_Contrib', 'Keywords','Text', 'Cleaned_Text']\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df_dominant_topic.head(4))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1cc030ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sec_redt.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "958c661f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(698, 6)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_dominant_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0798dab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9    21\n",
      "7    19\n",
      "3    19\n",
      "6    18\n",
      "1    14\n",
      "2    11\n",
      "5    10\n",
      "8    10\n",
      "0     9\n",
      "4     3\n",
      "Name: Document_Topic, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_dominant_topic['Document_Topic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61764c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c00cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d767a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87760412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5061a9ab",
   "metadata": {},
   "source": [
    "### Sentiment analysis *** start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "45e05f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import itertools\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06d67feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert classifier class\n",
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=False)\n",
    "        #outputs = self.bert(**inputs)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7b50ef9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentimentClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (drop): Dropout(p=0.3, inplace=False)\n",
       "  (out): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doing the sentiment analysis on one topic in particular\n",
    "\n",
    "# bert model\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "class_names = ['negative', 'neutral', 'positive']\n",
    "\n",
    "# moving the pytorch model to the GPU\n",
    "model1 = SentimentClassifier(3)\n",
    "model1 = model1.to(device)\n",
    "\n",
    "# importing the bert model\n",
    "model1.load_state_dict(torch.load('bert_models/bert_model_state_our_cleaning_balanced.bin'))\n",
    "#model1.load_state_dict(torch.load('best_model_state.bin'))\n",
    "#model1.load_state_dict(torch.load('best_model_state_unbalanced.bin'))\n",
    "model1.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e0de8d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vickyyounang/opt/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# getting the dataset on the topic we want\n",
    "# will do topic 1 \n",
    "\n",
    "dataset= pd.DataFrame(df_dominant_topic.loc[df_dominant_topic['Document_Topic']==1])\n",
    "# dataset.head(3)\n",
    "\n",
    "# encoding the texxxt and predicting\n",
    "#review_text = \"I love completing my todos! Best app ever!!!\"\n",
    "predictions=[]\n",
    "#i=0\n",
    "for item in dataset.Cleaned_Text.to_numpy():\n",
    "    #i+=1\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "    #review_text,\n",
    "    item,\n",
    "    #max_length=MAX_LEN,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    # predictions from our model\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "    \n",
    "    if input_ids.shape[1] <=512:\n",
    "        output = model1(input_ids, attention_mask)\n",
    "        _, prediction = torch.max(output, dim=1)\n",
    "        predictions.append( int(prediction.item()) )\n",
    "#print(i)\n",
    "#print(f'Review text: {review_text}')\n",
    "#print(f'Sentiment  : {class_names[prediction]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0ed1be3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "72c00061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vickyyounang/opt/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vizualising the preds\n",
    "plt.figure(figsize=(15,15))\n",
    "ax = sns.countplot(predictions, color = 'white', edgecolor = 'black')\n",
    "plt.xlabel('sentiment')\n",
    "# ax.set_xticklabels(class_names);\n",
    "plt.rc('xtick', labelsize = 20)\n",
    "plt.rc('ytick', labelsize = 20)\n",
    "\n",
    "num_locations = len(predictions)\n",
    "hatches = itertools.cycle(['-', '//', '+'])\n",
    "for i, bar in enumerate(ax.patches):\n",
    "    hatch = next(hatches)\n",
    "    bar.set_hatch(hatch)\n",
    "# plt.savefig(filename + ' cleaning_Balanced_Presentation.jpg')\n",
    "#plt.savefig(filename + ' state.jpg')\n",
    "#plt.savefig(filename + ' unbalanced.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8af397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a903f787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a631ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf87d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a35952a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405eb30c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2454b8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "125cd90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyLDAvis==3.3.1\n",
    "# print(pyLDAvis.__version__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f5c66f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pyLDAVis\n",
    "import pyLDAvis.gensim_models\n",
    "from IPython.core.display import display,HTML\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c93b71f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vickyyounang/opt/anaconda3/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n",
      "/Users/vickyyounang/opt/anaconda3/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/vickyyounang/opt/anaconda3/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/vickyyounang/opt/anaconda3/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/vickyyounang/opt/anaconda3/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "vis = pyLDAvis.gensim_models.prepare(lda_train4, train_corpus4_redt, dictionary= train_id2word_redt, mds = 'pcoa', sort_topics=True)\n",
    "# pyLDAvis.display(vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3a169888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el4952140535151431008407015677\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el4952140535151431008407015677_data = {\"mdsDat\": {\"x\": [0.03731789893410592, -0.014997322980003953, -0.041366763525674415, 0.15103160920522937, 0.034026954055864256, 0.06298384128169618, -0.25263764911896935, 0.014097206500646758, -0.04567473053898189, 0.055218956186086667], \"y\": [-0.04468142707706461, 0.014512839484597268, 0.0017557087760077619, 0.04724658733205147, -0.14403859630587781, 0.1625483633911737, 0.015464966116484312, -0.1564637537847382, 0.10036067626180999, 0.003294635805556213], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [19.726745140417805, 16.098900092840086, 12.044079211374862, 10.874776577383805, 8.69296554928051, 8.167947249211988, 6.42402759560362, 6.149752020164176, 6.106903984484996, 5.71390257923815]}, \"tinfo\": {\"Term\": [\"attack\", \"tesla\", \"technology\", \"security\", \"human\", \"could\", \"happen\", \"also\", \"safety\", \"year\", \"slow\", \"tech\", \"imagine\", \"automate_vehicle\", \"control\", \"know\", \"heart\", \"road\", \"think\", \"one\", \"long\", \"every\", \"much\", \"system\", \"big\", \"hacker\", \"life\", \"see\", \"people\", \"put\", \"never\", \"right\", \"leave\", \"enough\", \"computer\", \"point\", \"happen\", \"see\", \"long\", \"look\", \"much\", \"still\", \"need\", \"take\", \"slow\", \"way\", \"little\", \"even\", \"high\", \"want\", \"idea\", \"without\", \"future\", \"feel\", \"secure\", \"heart\", \"one\", \"good\", \"sure\", \"know\", \"people\", \"drive\", \"time\", \"well\", \"think\", \"might\", \"become\", \"pull\", \"big\", \"problem\", \"road\", \"public\", \"good\", \"could\", \"number\", \"bad\", \"also\", \"datum\", \"hand\", \"driver\", \"lot\", \"little\", \"year\", \"system\", \"much\", \"know\", \"use\", \"death\", \"sure\", \"since\", \"software\", \"drive\", \"work\", \"people\", \"point\", \"secure\", \"want\", \"see\", \"think\", \"human\", \"automate_vehicle\", \"human\", \"death\", \"yes\", \"vulnerable\", \"able\", \"time\", \"still\", \"cause\", \"though\", \"someone\", \"system\", \"end\", \"keep\", \"enough\", \"way\", \"every\", \"take\", \"high\", \"software\", \"could\", \"drive\", \"thing\", \"future\", \"work\", \"road\", \"rather\", \"leave\", \"start\", \"attack\", \"say\", \"even\", \"life\", \"see\", \"automate_vehicle\", \"well\", \"something\", \"driver\", \"technology\", \"though\", \"slow\", \"rather\", \"yes\", \"many\", \"issue\", \"hand\", \"number\", \"let\", \"around\", \"life\", \"mean\", \"since\", \"thing\", \"end\", \"well\", \"driver\", \"little\", \"use\", \"point\", \"something\", \"feel\", \"public\", \"happen\", \"problem\", \"secure\", \"enough\", \"control\", \"start\", \"drive\", \"people\", \"big\", \"even\", \"need\", \"say\", \"safety\", \"tesla\", \"put\", \"automate\", \"datum\", \"bad\", \"say\", \"many\", \"accident\", \"vulnerable\", \"someone\", \"well\", \"public\", \"want\", \"high\", \"even\", \"use\", \"tech\", \"also\", \"look\", \"good\", \"year\", \"system\", \"hand\", \"automate_vehicle\", \"technology\", \"point\", \"see\", \"never\", \"drive\", \"think\", \"people\", \"something\", \"security\", \"hacker\", \"issue\", \"kill\", \"life\", \"keep\", \"around\", \"idea\", \"fuck\", \"work\", \"think\", \"people\", \"use\", \"big\", \"let\", \"secure\", \"lot\", \"time\", \"hand\", \"feel\", \"automate_vehicle\", \"know\", \"attack\", \"high\", \"many\", \"problem\", \"system\", \"something\", \"say\", \"long\", \"need\", \"even\", \"take\", \"driver\", \"year\", \"imagine\", \"heart\", \"attack\", \"could\", \"future\", \"also\", \"one\", \"something\", \"sure\", \"tesla\", \"hacker\", \"without\", \"cause\", \"see\", \"pull\", \"look\", \"drive\", \"driver\", \"accident\", \"need\", \"thing\", \"datum\", \"number\", \"think\", \"able\", \"little\", \"automate_vehicle\", \"right\", \"know\", \"take\", \"people\", \"tech\", \"accident\", \"year\", \"vulnerable\", \"without\", \"long\", \"much\", \"lot\", \"fuck\", \"computer\", \"sure\", \"since\", \"leave\", \"also\", \"automate\", \"kill\", \"one\", \"look\", \"every\", \"end\", \"still\", \"let\", \"way\", \"need\", \"rather\", \"well\", \"someone\", \"technology\", \"slow\", \"human\", \"drive\", \"people\", \"even\", \"attack\", \"control\", \"mean\", \"start\", \"know\", \"software\", \"feel\", \"able\", \"tech\", \"attack\", \"public\", \"thing\", \"long\", \"automate_vehicle\", \"human\", \"imagine\", \"bad\", \"rather\", \"think\", \"system\", \"idea\", \"right\", \"every\", \"issue\", \"driver\", \"need\", \"kill\", \"something\", \"datum\", \"well\", \"time\", \"want\", \"technology\", \"people\", \"every\", \"software\", \"happen\", \"put\", \"road\", \"automate_vehicle\", \"tech\", \"think\", \"cause\", \"fuck\", \"secure\", \"kill\", \"one\", \"future\", \"someone\", \"end\", \"good\", \"time\", \"want\", \"high\", \"rather\", \"year\", \"never\", \"system\", \"sure\", \"problem\", \"driver\", \"number\", \"thing\", \"people\", \"even\", \"something\", \"much\", \"still\", \"drive\"], \"Freq\": [44.0, 29.0, 33.0, 18.0, 38.0, 29.0, 26.0, 30.0, 16.0, 30.0, 29.0, 20.0, 12.0, 37.0, 16.0, 33.0, 12.0, 28.0, 42.0, 28.0, 20.0, 19.0, 23.0, 33.0, 21.0, 13.0, 22.0, 37.0, 70.0, 13.0, 14.955305860565257, 13.212842370429641, 9.598138224070775, 8.965576633312084, 6.086562419508309, 11.242367331077233, 13.179181427607162, 17.922723776295964, 9.571724793682218, 8.80453253047599, 10.520056324967642, 8.47357255591542, 16.76370275905411, 15.003662163491894, 11.868719300030167, 14.185425252591653, 5.269731707863001, 17.502577636606606, 3.766548238727065, 8.106894349759399, 6.157644828695019, 6.476673298892497, 4.779192389251886, 3.96749642381747, 9.517608852711401, 3.9970867087419677, 7.993137838416192, 7.817679615326927, 3.13342301248619, 8.631133907720114, 17.09954937234059, 11.102471694515199, 6.23803126856848, 6.295464684000729, 5.8830206636277085, 10.090761075972004, 10.603770687752927, 9.565666216580276, 14.32790711684009, 8.63661697241705, 12.036299182450948, 5.907150059849921, 11.840251426682405, 11.249111214160422, 5.391868529674812, 4.373635955843159, 10.361796601556023, 5.262662839077883, 4.453473796672845, 12.025125091705124, 5.410556363263734, 3.8396423825972086, 8.868200474737534, 8.946378236807009, 6.147848274908769, 8.552610698973178, 8.378252752551608, 2.637122830668319, 2.6899997976785026, 2.7388991782501546, 2.2812013156610593, 15.669781477240843, 3.1111422857953217, 14.298029472062538, 4.430560622466784, 5.489972267408533, 4.504307734936085, 5.923624455091892, 6.152684511933312, 5.601607533491123, 5.481448014501927, 21.20632138270381, 5.7485644215388705, 6.116407695614413, 4.42513552969552, 8.9415773813904, 11.04329942751627, 6.542451175916773, 8.385969699517165, 3.725130979216464, 6.933977697634693, 10.511366995843664, 4.893175458367717, 3.1885776727456294, 4.677705089728394, 8.507012391686652, 4.550586223581414, 7.7775868995487665, 2.2483504361982627, 2.2175798086350174, 5.950844186109214, 15.120238104446877, 4.8430004497161585, 2.776559823518291, 2.7091369204238838, 4.933087867505571, 1.8657890466798701, 2.820802141706071, 1.925905581080839, 6.976881149986305, 4.05764837699664, 6.576267182253888, 3.271495384007945, 4.682452747500217, 4.327799136618155, 3.574277587712369, 2.865193353441689, 2.8551575049234272, 23.468097561483777, 6.367648378915096, 13.28220910664179, 4.888139450258072, 6.286732311986217, 7.445415678597588, 5.526595490697023, 4.626501092573334, 4.533141529819444, 6.039489246109285, 3.3882082367138415, 5.517527178958905, 3.1264513695450558, 3.0677452117457324, 5.813421998541426, 3.285375561009297, 7.420149796410752, 7.355686583801246, 2.492747389510349, 6.414892036062913, 4.055098482785908, 4.703639722793838, 2.127828441243187, 2.486929805662615, 4.703583653790632, 2.775139561302005, 4.544165460426938, 2.5533569200624244, 2.3990401564600674, 1.6490038262861817, 10.239659478446299, 8.636456382005747, 2.7402954367835046, 2.7237869382261026, 2.669673553890099, 2.564278482262551, 13.441568336050745, 20.410379471827788, 6.291871054936748, 8.427274355402785, 5.179863279960738, 3.8444404100785947, 8.961654699946799, 5.367342358111953, 3.3371842270906678, 2.283369941131395, 4.693503675444674, 7.470692331565622, 2.737917295129818, 4.415069152187597, 1.792159266361565, 7.452303288183286, 5.626911117706979, 3.289119346296079, 4.386480875356488, 2.6713963109420353, 3.825862240488423, 3.5225598279392947, 3.7955253102096425, 1.464687916582916, 4.123457930999682, 3.522543190265715, 2.106539653159966, 3.448469211589244, 2.004875488294995, 6.569743971824417, 3.6344111724283015, 4.317796823831153, 2.140775571310517, 15.964909403879235, 7.981937223410399, 6.155973077250458, 6.660399789803726, 7.9851263204375655, 3.7093208725027584, 3.641419203076698, 5.219679266407581, 3.8041240875898525, 3.325676264774993, 7.353688299431802, 12.118584017023426, 5.850891089955135, 3.572654218669094, 2.9278156732103424, 4.464179436950937, 2.7048963234397525, 4.38903912982486, 1.8704263033773776, 1.6569381752723118, 5.184209654543908, 4.400665806791749, 5.082032210896798, 1.1581048249719732, 2.018892392885435, 1.878370013549388, 3.5763518611805845, 2.468312924391801, 2.4892420914665445, 1.5700343241700754, 2.7632090072467137, 2.9480231778725763, 2.05549905374919, 1.9792465825338064, 1.8847654356188734, 9.623441429460708, 7.742756952494946, 18.038221407545684, 9.77836871314196, 3.803292984089708, 7.8652216612713035, 6.236184242615702, 5.368339429558111, 2.2284799641703086, 4.770091516033774, 2.0334879704419118, 2.7226169919738736, 3.406578558935432, 4.66067868241203, 1.504399237436356, 2.1427244070274805, 8.516325453709225, 3.616856121155615, 1.3337103892780768, 3.4247788187105925, 2.0570061115629907, 1.2718942685959267, 1.1238040637791187, 3.2592544140780024, 1.5614067197098815, 0.8903901985456797, 2.4696124543383804, 1.2718888738074803, 1.8517879826840877, 1.7431050764851517, 1.2114987337179108, 7.886629047931345, 4.78833082521734, 9.551161245631857, 2.9333839149014684, 5.306625111591061, 5.263816331049049, 4.945559941326207, 3.551317381891575, 2.566337823581739, 2.101184353467663, 2.1013227128607155, 2.103311148721282, 2.7400771066396685, 4.857185281086879, 2.9975865165941853, 2.4802492160224086, 4.07689307153195, 2.6389933194437414, 2.537743789224887, 2.1024615355733207, 2.4251547939577387, 2.329923416922098, 4.2990756772895775, 4.569085954489276, 1.2402855468885392, 4.104566871800636, 2.101035665161588, 2.4964233329873045, 2.109519305862663, 2.3086545035567885, 4.388243982380719, 4.065415487297884, 2.3908640037577844, 2.3094214854005664, 8.750963778425275, 5.374531896366523, 3.5498420287103887, 7.901199101679404, 2.5030774200450976, 2.5450047181522155, 4.3448007370787245, 3.5734683096265454, 7.796808666511996, 2.4396360564881885, 3.907788724094818, 3.03760175041294, 5.659219421714073, 5.532997432809383, 1.7957772039996514, 1.5735902654953637, 1.2546934368040752, 4.6329646518341585, 3.4448822137216126, 1.78631147318635, 1.930250310374364, 1.7446281950026756, 1.2546803174533525, 3.161623982001896, 3.0808264330397144, 1.3950048927840688, 1.9919702958496215, 1.139823860613445, 2.6534614364427327, 2.1422334949537825, 1.6500641535266203, 2.0109312163819375, 2.490137208093109, 6.329665472520008, 2.9871066418047865, 7.476863437895045, 3.5608654236238517, 5.650725831037694, 7.397777597866885, 3.495700409058129, 7.23941665850393, 3.806298365478853, 1.9529752661418058, 4.006469988788358, 2.3033426734683973, 3.794783900681693, 1.8648753936772198, 2.801292167005448, 1.952947591431873, 3.3657352843681347, 3.3823119892515328, 2.43437617587168, 1.0333203281154368, 1.0320511971635635, 2.8738530246175396, 1.9889133320040069, 3.004783862774298, 1.0320520898961418, 1.3905280756414422, 2.8628862512572737, 1.0320568883337513, 1.7864204923988967, 5.1204270566039085, 2.5821562255314157, 1.556364088067107, 1.108663944766205, 1.0566276786826683, 1.080019504070138], \"Total\": [44.0, 29.0, 33.0, 18.0, 38.0, 29.0, 26.0, 30.0, 16.0, 30.0, 29.0, 20.0, 12.0, 37.0, 16.0, 33.0, 12.0, 28.0, 42.0, 28.0, 20.0, 19.0, 23.0, 33.0, 21.0, 13.0, 22.0, 37.0, 70.0, 13.0, 21.71320421953948, 20.183025269076225, 16.696072541024744, 17.14343198265381, 12.021347349756343, 22.494532905452225, 26.969933956889133, 37.254025007591075, 20.14279732686752, 18.808107093833993, 23.27556427879177, 19.115817827802324, 38.057563150987995, 35.579441146515286, 29.148132405395252, 35.008193234603304, 13.167603710100591, 44.41471155305018, 10.48038016922478, 23.658918621110235, 18.208630384192997, 19.240699415499304, 14.30037152792741, 12.069177164125518, 29.13526708346574, 12.529301558637457, 28.864765151428042, 29.890365831086925, 12.058253556831525, 33.45980549435278, 70.17529966656522, 75.0751009936673, 30.47700651684006, 36.52150278962304, 42.43138295842018, 11.436983757819474, 14.173958734339536, 13.153508541788032, 21.484062639937015, 17.62735380684864, 28.221424605769204, 14.162947032503421, 29.890365831086925, 29.15281629859346, 14.0806475583707, 11.62987156165049, 30.003422032426634, 15.392489275963754, 13.109499123000608, 36.57796479657105, 17.800558767647587, 13.167603710100591, 30.927850888806486, 33.801918588441886, 23.27556427879177, 33.45980549435278, 34.23010387784715, 11.710436970066601, 12.058253556831525, 12.479318334727374, 10.740754591839933, 75.0751009936673, 15.265241498838725, 70.17529966656522, 22.494532905452225, 29.13526708346574, 23.658918621110235, 37.254025007591075, 42.43138295842018, 38.23392488719556, 37.79255601553824, 38.23392488719556, 11.710436970066601, 14.084474117371748, 10.345657549630452, 22.417750741313917, 30.47700651684006, 19.115817827802324, 25.970501724721217, 11.606924707674775, 22.160222655043242, 33.801918588441886, 16.060847663695625, 10.76135697566629, 17.14343198265381, 35.008193234603304, 19.101901003988665, 35.579441146515286, 10.48038016922478, 10.740754591839933, 29.15281629859346, 75.0751009936673, 24.373501920315146, 14.30037152792741, 15.265241498838725, 28.221424605769204, 10.848829310795567, 16.696072541024744, 11.536399581400085, 44.758715622424425, 27.149868275562852, 44.41471155305018, 22.13562863967093, 37.254025007591075, 37.79255601553824, 36.52150278962304, 26.37430531924042, 36.57796479657105, 33.551507666890345, 11.606924707674775, 29.148132405395252, 10.848829310795567, 14.084474117371748, 18.6035488590419, 13.850914349449877, 13.109499123000608, 14.0806475583707, 18.85760793385098, 11.046395146535135, 22.13562863967093, 12.650091760026744, 12.479318334727374, 24.373501920315146, 16.060847663695625, 36.52150278962304, 36.57796479657105, 13.167603710100591, 34.23010387784715, 22.494532905452225, 26.37430531924042, 12.069177164125518, 14.162947032503421, 26.969933956889133, 17.62735380684864, 29.13526708346574, 17.14343198265381, 16.23086564153518, 11.536399581400085, 75.0751009936673, 70.17529966656522, 21.484062639937015, 44.41471155305018, 38.057563150987995, 27.149868275562852, 16.596745700684476, 29.40832881790937, 13.402786565890386, 19.050812312441554, 15.392489275963754, 11.62987156165049, 27.149868275562852, 18.6035488590419, 14.309164479954651, 10.345657549630452, 22.160222655043242, 36.52150278962304, 14.162947032503421, 23.658918621110235, 10.48038016922478, 44.41471155305018, 34.23010387784715, 20.383509680589164, 30.003422032426634, 18.808107093833993, 29.890365831086925, 30.927850888806486, 33.801918588441886, 13.109499123000608, 37.79255601553824, 33.551507666890345, 22.494532905452225, 37.254025007591075, 21.71320421953948, 75.0751009936673, 42.43138295842018, 70.17529966656522, 26.37430531924042, 18.91674772515841, 13.322062507782764, 13.850914349449877, 17.283563058382683, 22.13562863967093, 10.76135697566629, 11.046395146535135, 18.208630384192997, 14.202097651635372, 15.265241498838725, 42.43138295842018, 70.17529966656522, 34.23010387784715, 21.484062639937015, 18.85760793385098, 29.13526708346574, 17.800558767647587, 30.47700651684006, 13.109499123000608, 12.069177164125518, 37.79255601553824, 33.45980549435278, 44.758715622424425, 10.48038016922478, 18.6035488590419, 17.62735380684864, 33.801918588441886, 26.37430531924042, 27.149868275562852, 20.14279732686752, 38.057563150987995, 44.41471155305018, 35.579441146515286, 36.57796479657105, 30.927850888806486, 12.686987730554918, 12.529301558637457, 44.758715622424425, 29.15281629859346, 14.30037152792741, 30.003422032426634, 28.864765151428042, 26.37430531924042, 12.058253556831525, 29.40832881790937, 13.322062507782764, 19.240699415499304, 25.970501724721217, 37.254025007591075, 13.153508541788032, 18.808107093833993, 75.0751009936673, 36.57796479657105, 14.309164479954651, 38.057563150987995, 24.373501920315146, 15.392489275963754, 14.0806475583707, 42.43138295842018, 22.417750741313917, 13.167603710100591, 37.79255601553824, 20.183025269076225, 33.45980549435278, 35.579441146515286, 70.17529966656522, 20.383509680589164, 14.309164479954651, 30.927850888806486, 10.345657549630452, 19.240699415499304, 20.14279732686752, 23.27556427879177, 17.800558767647587, 14.202097651635372, 12.021347349756343, 12.058253556831525, 12.479318334727374, 16.696072541024744, 30.003422032426634, 19.050812312441554, 17.283563058382683, 28.864765151428042, 18.808107093833993, 19.101901003988665, 16.060847663695625, 19.115817827802324, 18.85760793385098, 35.008193234603304, 38.057563150987995, 10.848829310795567, 36.52150278962304, 22.160222655043242, 33.551507666890345, 29.148132405395252, 38.23392488719556, 75.0751009936673, 70.17529966656522, 44.41471155305018, 44.758715622424425, 16.23086564153518, 12.650091760026744, 11.536399581400085, 33.45980549435278, 10.740754591839933, 12.069177164125518, 22.417750741313917, 20.383509680589164, 44.758715622424425, 14.162947032503421, 24.373501920315146, 20.14279732686752, 37.79255601553824, 38.23392488719556, 12.686987730554918, 11.62987156165049, 10.848829310795567, 42.43138295842018, 33.801918588441886, 18.208630384192997, 20.183025269076225, 19.101901003988665, 13.850914349449877, 36.57796479657105, 38.057563150987995, 17.283563058382683, 26.37430531924042, 15.392489275963754, 36.52150278962304, 30.47700651684006, 23.658918621110235, 33.551507666890345, 70.17529966656522, 19.101901003988665, 10.740754591839933, 26.969933956889133, 13.402786565890386, 28.221424605769204, 37.79255601553824, 20.383509680589164, 42.43138295842018, 25.970501724721217, 14.202097651635372, 29.13526708346574, 17.283563058382683, 28.864765151428042, 14.30037152792741, 22.160222655043242, 16.060847663695625, 29.890365831086925, 30.47700651684006, 23.658918621110235, 10.48038016922478, 10.848829310795567, 30.927850888806486, 21.71320421953948, 33.801918588441886, 12.058253556831525, 17.62735380684864, 36.57796479657105, 14.0806475583707, 24.373501920315146, 70.17529966656522, 44.41471155305018, 26.37430531924042, 23.27556427879177, 19.115817827802324, 75.0751009936673], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.32, -3.4439, -3.7635, -3.8317, -4.219, -3.6054, -3.4464, -3.139, -3.7663, -3.8498, -3.6718, -3.8881, -3.2059, -3.3168, -3.5512, -3.3729, -4.3631, -3.1627, -4.6989, -3.9324, -4.2074, -4.1569, -4.4608, -4.6469, -3.7719, -4.6395, -3.9465, -3.9687, -4.8829, -3.8697, -3.186, -3.6179, -4.1944, -4.1852, -4.253, -3.5102, -3.4606, -3.5637, -3.1596, -3.6658, -3.3339, -4.0457, -3.3503, -3.4016, -4.137, -4.3462, -3.4837, -4.1612, -4.3282, -3.3348, -4.1335, -4.4765, -3.6394, -3.6306, -4.0057, -3.6756, -3.6962, -4.8522, -4.8323, -4.8143, -4.9971, -3.0701, -4.6869, -3.1617, -4.3333, -4.1189, -4.3168, -4.0429, -4.005, -4.0988, -4.1205, -2.4774, -3.7827, -3.7207, -4.0444, -3.341, -3.1298, -3.6534, -3.4051, -4.2166, -3.5952, -3.1792, -3.9438, -4.3721, -3.9889, -3.3908, -4.0164, -3.4804, -4.7215, -4.7352, -3.7481, -2.8156, -3.9541, -4.5105, -4.535, -3.9357, -4.908, -4.4946, -4.8763, -3.5891, -4.1311, -3.6482, -4.3464, -3.9878, -4.0666, -4.2579, -4.479, -4.4825, -2.2739, -3.5783, -2.8431, -3.8427, -3.5911, -3.4219, -3.72, -3.8977, -3.9181, -3.6312, -4.2092, -3.7216, -4.2896, -4.3086, -3.6694, -4.2401, -3.4253, -3.4341, -4.5162, -3.5709, -4.0296, -3.8812, -4.6744, -4.5185, -3.8812, -4.4088, -3.9157, -4.4921, -4.5545, -4.9294, -3.1033, -3.2735, -4.4215, -4.4275, -4.4476, -4.4879, -2.6073, -2.1896, -3.3663, -3.0741, -3.5608, -3.859, -3.0127, -3.5253, -4.0005, -4.38, -3.6594, -3.1946, -4.1984, -3.7206, -4.6222, -3.1971, -3.478, -4.015, -3.7271, -4.223, -3.8638, -3.9464, -3.8718, -4.824, -3.7889, -3.9464, -4.4606, -3.9677, -4.51, -3.3231, -3.9152, -3.7429, -4.4444, -2.3729, -3.0661, -3.3259, -3.2471, -3.0657, -3.8325, -3.8509, -3.4909, -3.8072, -3.9416, -3.1481, -2.6486, -3.3767, -3.87, -4.0691, -3.6472, -4.1482, -3.6642, -4.5171, -4.6383, -3.4977, -3.6616, -3.5176, -4.9965, -4.4408, -4.5129, -3.869, -4.2398, -4.2313, -4.6922, -4.1269, -4.0622, -4.4228, -4.4606, -4.5095, -2.6389, -2.8564, -2.0106, -2.623, -3.5673, -2.8407, -3.0728, -3.2226, -4.1018, -3.3408, -4.1934, -3.9015, -3.6774, -3.364, -4.4947, -4.1411, -2.7612, -3.6175, -4.6152, -3.6721, -4.1819, -4.6626, -4.7864, -3.7216, -4.4576, -5.0192, -3.9991, -4.6626, -4.287, -4.3475, -4.7113, -2.7943, -3.2933, -2.6028, -3.7833, -3.1905, -3.1986, -3.261, -3.5922, -3.917, -4.117, -4.1169, -4.116, -3.8515, -3.279, -3.7617, -3.9511, -3.4542, -3.8891, -3.9282, -4.1164, -3.9736, -4.0137, -3.4011, -3.3402, -4.6442, -3.4474, -4.1171, -3.9446, -4.113, -4.0228, -3.3806, -3.457, -3.9878, -4.0225, -2.6833, -3.1708, -3.5856, -2.7855, -3.935, -3.9184, -3.3835, -3.579, -2.7988, -3.9607, -3.4895, -3.7414, -3.1192, -3.1418, -4.2671, -4.3992, -4.6256, -3.3193, -3.6156, -4.2724, -4.1949, -4.296, -4.6256, -3.7014, -3.7273, -4.5196, -4.1634, -4.7216, -3.8766, -4.0907, -4.3517, -4.1539, -3.9402, -2.9407, -3.6917, -2.7742, -3.516, -3.0542, -2.7848, -3.5345, -2.8065, -3.4493, -4.1166, -3.3981, -3.9516, -3.4524, -4.1628, -3.7559, -4.1167, -3.5723, -3.5674, -3.8963, -4.7532, -4.7544, -3.7303, -4.0984, -3.6858, -4.7544, -4.4563, -3.7342, -4.7544, -4.2058, -3.1528, -3.8374, -4.3436, -4.6828, -4.7309, -4.709], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2503, 1.1995, 1.0696, 0.975, 0.9426, 0.9296, 0.9071, 0.8915, 0.8792, 0.8642, 0.8291, 0.8096, 0.8033, 0.7597, 0.7247, 0.7198, 0.7074, 0.692, 0.5998, 0.5522, 0.539, 0.5344, 0.5272, 0.5107, 0.5044, 0.4807, 0.3392, 0.282, 0.2756, 0.2682, 0.2113, -0.2881, 0.0369, -0.1349, -0.3526, 1.7012, 1.5362, 1.5079, 1.4213, 1.113, 0.9743, 0.952, 0.9004, 0.8742, 0.8665, 0.8484, 0.7632, 0.7532, 0.7468, 0.714, 0.6355, 0.594, 0.5772, 0.4972, 0.4951, 0.4623, 0.419, 0.3356, 0.3262, 0.3099, 0.2771, 0.2597, 0.2358, 0.2355, 0.2017, 0.1574, 0.1677, -0.0124, -0.1046, -0.0943, -0.1043, 1.5272, 1.4051, 1.2825, 1.2673, 1.1975, 1.1014, 1.0444, 0.9862, 0.9801, 0.9547, 0.9485, 0.9281, 0.9002, 0.8178, 0.7019, 0.6821, 0.5961, 0.5773, 0.539, 0.5276, 0.5141, 0.5006, 0.4775, 0.3876, 0.3725, 0.3562, 0.3384, 0.3265, 0.2579, 0.2158, 0.2065, 0.2047, 0.0427, -0.0505, -0.2075, -0.1032, -0.4337, 1.8613, 1.6184, 1.4328, 1.4215, 1.4121, 1.303, 1.2999, 1.1772, 1.0853, 1.0801, 1.0369, 0.8295, 0.821, 0.8156, 0.7854, 0.6318, 0.625, 0.6148, 0.5544, 0.5442, 0.5054, 0.4947, 0.4832, 0.4791, 0.4723, 0.37, 0.3606, 0.3145, 0.3069, 0.2734, 0.2265, 0.1237, 0.1595, -0.5728, -0.4384, -0.141, 2.2318, 2.0774, 1.6865, 1.627, 1.3536, 1.3357, 1.3342, 1.1996, 0.9869, 0.9317, 0.8905, 0.8557, 0.7992, 0.7639, 0.6766, 0.6576, 0.6371, 0.6185, 0.5199, 0.491, 0.3869, 0.2702, 0.256, 0.251, 0.2272, 0.1888, 0.0744, 0.0628, 0.0603, 0.0066, -0.0148, -0.3456, -0.0686, 2.3353, 1.9927, 1.694, 1.5514, 1.4853, 1.4398, 1.3952, 1.2555, 1.1876, 0.981, 0.7523, 0.7487, 0.7384, 0.7109, 0.6423, 0.6291, 0.6208, 0.5671, 0.5578, 0.5193, 0.5185, 0.4764, 0.3294, 0.3022, 0.2841, 0.2659, 0.2588, 0.1361, 0.1156, -0.0468, -0.1178, -0.2075, -0.3463, -0.4118, -0.2929, 2.4687, 2.2638, 1.8363, 1.6527, 1.4207, 1.4063, 1.2129, 1.1533, 1.0567, 0.9262, 0.8655, 0.7897, 0.7139, 0.6665, 0.5768, 0.5729, 0.5686, 0.4313, 0.3722, 0.3371, 0.2729, 0.2518, 0.217, 0.1787, 0.0809, 0.0513, 0.0171, -0.0192, -0.1491, -0.271, -1.314, 1.8392, 1.694, 1.6138, 1.5283, 1.5007, 1.4468, 1.2398, 1.1768, 1.0778, 1.0446, 1.0416, 1.0082, 0.9816, 0.9679, 0.9395, 0.8474, 0.8315, 0.8249, 0.7702, 0.7555, 0.7241, 0.6977, 0.6916, 0.669, 0.62, 0.603, 0.4329, 0.1905, 0.1628, -0.0183, -0.0508, -0.0597, -0.1332, -0.1755, 2.178, 1.9398, 1.6171, 1.3524, 1.3392, 1.2392, 1.1549, 1.0546, 1.0482, 1.037, 0.9652, 0.904, 0.8969, 0.8628, 0.8406, 0.7955, 0.6386, 0.5811, 0.5121, 0.474, 0.4486, 0.4025, 0.3943, 0.3474, 0.2818, 0.2789, 0.2125, 0.1927, 0.1737, 0.1406, 0.1328, -0.0187, -0.5429, 1.7577, 1.5825, 1.5794, 1.5368, 1.254, 1.2313, 1.0991, 1.0939, 0.942, 0.8782, 0.8782, 0.8469, 0.8333, 0.8252, 0.794, 0.7552, 0.6784, 0.6639, 0.5882, 0.5455, 0.5098, 0.4863, 0.4719, 0.442, 0.4041, 0.3225, 0.3147, 0.249, 0.249, 0.2445, 0.0173, 0.0322, -0.182, -0.0332, -1.3792]}, \"token.table\": {\"Topic\": [1, 3, 6, 7, 8, 9, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 7, 8, 1, 3, 4, 6, 1, 2, 3, 6, 7, 8, 9, 2, 3, 4, 5, 7, 8, 9, 1, 2, 3, 5, 6, 7, 9, 10, 1, 2, 5, 9, 2, 3, 6, 10, 2, 4, 6, 1, 2, 3, 4, 5, 6, 7, 10, 1, 2, 3, 8, 1, 3, 4, 9, 2, 3, 7, 9, 2, 4, 5, 7, 9, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 9, 10, 1, 2, 3, 4, 8, 10, 1, 3, 4, 1, 2, 3, 4, 5, 6, 8, 10, 2, 3, 8, 9, 10, 1, 4, 5, 6, 9, 1, 2, 5, 6, 8, 10, 1, 2, 3, 7, 10, 1, 2, 3, 5, 9, 10, 2, 6, 7, 2, 4, 5, 6, 1, 4, 9, 10, 1, 7, 1, 3, 5, 6, 10, 2, 3, 4, 6, 8, 9, 1, 3, 4, 6, 9, 2, 7, 9, 4, 6, 9, 1, 2, 3, 6, 1, 3, 6, 8, 9, 10, 1, 2, 3, 6, 7, 9, 10, 1, 3, 8, 9, 1, 2, 3, 4, 6, 8, 10, 2, 3, 4, 6, 9, 1, 2, 4, 7, 1, 6, 8, 9, 1, 2, 5, 6, 7, 8, 1, 2, 4, 6, 8, 9, 1, 4, 5, 6, 1, 3, 4, 9, 2, 1, 2, 8, 10, 1, 2, 3, 4, 6, 7, 8, 9, 1, 3, 4, 5, 10, 1, 2, 4, 6, 7, 10, 1, 2, 4, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 1, 2, 4, 6, 10, 2, 4, 5, 9, 2, 4, 7, 1, 3, 5, 10, 3, 4, 8, 9, 10, 1, 2, 7, 9, 1, 2, 3, 4, 10, 5, 6, 10, 1, 2, 3, 4, 5, 6, 9, 1, 2, 3, 4, 6, 10, 4, 6, 10, 1, 2, 3, 5, 7, 1, 2, 3, 4, 8, 1, 2, 4, 8, 2, 3, 9, 10, 1, 2, 3, 5, 8, 10, 1, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 9, 1, 3, 8, 10, 1, 2, 7, 8, 10, 2, 3, 5, 6, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 2, 5, 8, 9, 10, 1, 4, 5, 8, 9, 2, 3, 5, 7, 2, 3, 4, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 2, 3, 4, 1, 3, 4, 5, 6, 9, 10, 1, 2, 4, 5, 6, 8, 9, 3, 5, 8, 1, 2, 3, 5, 8, 9, 10, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 8, 9, 10, 1, 3, 6, 7, 8, 9, 1, 2, 3, 5, 6, 9, 2, 3, 4, 5, 6, 8, 10, 2, 3, 4], \"Freq\": [0.22303754099582548, 0.4014675737924859, 0.0446075081991651, 0.0892150163983302, 0.0446075081991651, 0.1784300327966604, 0.1397705647175801, 0.20965584707637017, 0.06988528235879005, 0.06988528235879005, 0.34942641179395023, 0.06988528235879005, 0.06988528235879005, 0.03332953150874708, 0.33329531508747084, 0.03332953150874708, 0.13331812603498833, 0.26663625206997665, 0.16664765754373542, 0.18105454073198982, 0.09052727036599491, 0.2715818110979847, 0.36210908146397963, 0.022342017327659714, 0.08936806931063886, 0.156394121293618, 0.11171008663829857, 0.4021563118978748, 0.04468403465531943, 0.1787361386212777, 0.10498240007822951, 0.052491200039114755, 0.10498240007822951, 0.41992960031291804, 0.052491200039114755, 0.15747360011734426, 0.052491200039114755, 0.07938071187263872, 0.13230118645439787, 0.10584094916351829, 0.10584094916351829, 0.13230118645439787, 0.05292047458175914, 0.15876142374527744, 0.185221661036157, 0.0859854723845361, 0.3439418895381444, 0.3439418895381444, 0.1719709447690722, 0.7760711178980703, 0.07055191980891548, 0.07055191980891548, 0.07055191980891548, 0.6516458378768274, 0.13963839383074872, 0.18618452510766495, 0.11551567358224396, 0.07701044905482932, 0.30804179621931727, 0.07701044905482932, 0.03850522452741466, 0.03850522452741466, 0.11551567358224396, 0.15402089810965863, 0.4991121066077183, 0.16637070220257277, 0.16637070220257277, 0.16637070220257277, 0.12322201687641056, 0.12322201687641056, 0.12322201687641056, 0.5544990759438475, 0.37732203596846725, 0.20581201961916396, 0.3430200326986066, 0.03430200326986066, 0.3248337491329478, 0.12993349965317913, 0.3248337491329478, 0.06496674982658956, 0.06496674982658956, 0.17078781988343048, 0.2561817298251457, 0.5123634596502914, 0.14651994941609026, 0.213119926423404, 0.19979993102194124, 0.1331999540146275, 0.09323996781023924, 0.01331999540146275, 0.11987995861316475, 0.053279981605851, 0.01331999540146275, 0.01331999540146275, 0.08201659159235755, 0.3280663663694302, 0.08201659159235755, 0.1913720470488343, 0.05467772772823837, 0.10935545545647674, 0.08201659159235755, 0.08201659159235755, 0.12452642860942228, 0.12452642860942228, 0.3113160715235557, 0.18678964291413344, 0.12452642860942228, 0.12452642860942228, 0.524982396121526, 0.2916568867341811, 0.17499413204050868, 0.4052711223509871, 0.04503012470566523, 0.1576054364698283, 0.06754518705849785, 0.1576054364698283, 0.06754518705849785, 0.04503012470566523, 0.06754518705849785, 0.15705243155503582, 0.2617540525917264, 0.15705243155503582, 0.10470162103669055, 0.31410486311007163, 0.33142275944789507, 0.16571137972394753, 0.08285568986197377, 0.16571137972394753, 0.24856706958592129, 0.1408242675876616, 0.1408242675876616, 0.0704121337938308, 0.2816485351753232, 0.2112364013814924, 0.1408242675876616, 0.3496412656297373, 0.06992825312594746, 0.20978475937784238, 0.2797130125037898, 0.1398565062518949, 0.2676447670533275, 0.40146715057999127, 0.03345559588166594, 0.13382238352666376, 0.03345559588166594, 0.10036678764499782, 0.22519035609143828, 0.6005076162438354, 0.15012690406095885, 0.30512226000930903, 0.38140282501163625, 0.07628056500232726, 0.15256113000465452, 0.48201823633607055, 0.18539162936002712, 0.03707832587200543, 0.259548281104038, 0.3192516343612528, 0.6385032687225056, 0.3816655441322483, 0.19083277206612415, 0.19083277206612415, 0.09541638603306207, 0.09541638603306207, 0.1569286966405425, 0.5492504382418987, 0.05230956554684749, 0.026154782773423747, 0.05230956554684749, 0.1569286966405425, 0.3295140751063095, 0.10983802503543649, 0.10983802503543649, 0.27459506258859123, 0.10983802503543649, 0.07882091645691698, 0.7882091645691698, 0.15764183291383396, 0.4331843984175893, 0.4331843984175893, 0.07219739973626488, 0.18585016782943117, 0.09292508391471559, 0.2787752517441468, 0.37170033565886235, 0.11571688043976454, 0.11571688043976454, 0.40500908153917586, 0.11571688043976454, 0.05785844021988227, 0.11571688043976454, 0.268979447639616, 0.268979447639616, 0.02988660529329067, 0.11954642117316268, 0.05977321058658134, 0.23909284234632536, 0.02988660529329067, 0.5989432529972846, 0.1796829758991854, 0.1796829758991854, 0.05989432529972846, 0.2651449758388806, 0.05302899516777611, 0.05302899516777611, 0.3181739710066567, 0.15908698550332834, 0.10605799033555222, 0.05302899516777611, 0.18070415189525263, 0.13552811392143946, 0.2710562278428789, 0.36140830379050526, 0.04517603797381316, 0.3797198115982641, 0.3037758492786113, 0.15188792463930564, 0.07594396231965282, 0.4964553749772121, 0.09929107499544242, 0.24822768748860605, 0.14893661249316362, 0.47851705411389006, 0.05316856156821, 0.15950568470463003, 0.05316856156821, 0.10633712313642, 0.15950568470463003, 0.16853403531649144, 0.2808900588608191, 0.11235602354432764, 0.16853403531649144, 0.22471204708865528, 0.05617801177216382, 0.1612595544393621, 0.3762722936918449, 0.2687659240656035, 0.1075063696262414, 0.23715242995151664, 0.07905080998383887, 0.23715242995151664, 0.3952540499191944, 0.8743564047787505, 0.47259863899510185, 0.25778107581551013, 0.21481756317959175, 0.042963512635918355, 0.44669176354132045, 0.07882795827199772, 0.05255197218133182, 0.07882795827199772, 0.07882795827199772, 0.07882795827199772, 0.13137993045332955, 0.07882795827199772, 0.6908238806367261, 0.04605492537578174, 0.04605492537578174, 0.09210985075156349, 0.09210985075156349, 0.07101946099101937, 0.35509730495509684, 0.35509730495509684, 0.07101946099101937, 0.07101946099101937, 0.07101946099101937, 0.27715451548041475, 0.10393294330515554, 0.06928862887010369, 0.034644314435051844, 0.20786588661031108, 0.13857725774020738, 0.13857725774020738, 0.24225047959573717, 0.19950039496119532, 0.014250028211513952, 0.12825025390362557, 0.05700011284605581, 0.1710003385381674, 0.014250028211513952, 0.05700011284605581, 0.028500056423027904, 0.07125014105756976, 0.4890077089501965, 0.17782098507279873, 0.17782098507279873, 0.08891049253639936, 0.11346002479526753, 0.5105701115787039, 0.1701900371929013, 0.11346002479526753, 0.05673001239763376, 0.42364064387378064, 0.14121354795792687, 0.21182032193689032, 0.14121354795792687, 0.760253431107792, 0.07602534311077919, 0.15205068622155837, 0.14922270008312516, 0.07461135004156258, 0.4476681002493755, 0.2984454001662503, 0.18435168834390445, 0.4608792208597611, 0.09217584417195222, 0.09217584417195222, 0.09217584417195222, 0.644105619781301, 0.14863975841106944, 0.049546586137023146, 0.09909317227404629, 0.0708681446078079, 0.4252088676468474, 0.17717036151951976, 0.0708681446078079, 0.2126044338234237, 0.78328608719141, 0.06025277593780077, 0.06025277593780077, 0.11049777367429257, 0.14733036489905676, 0.14733036489905676, 0.11049777367429257, 0.3314933210228777, 0.07366518244952838, 0.03683259122476419, 0.3432266459529042, 0.1716133229764521, 0.034322664595290424, 0.1716133229764521, 0.1372906583811617, 0.1372906583811617, 0.1057264192057756, 0.8458113536462049, 0.0528632096028878, 0.48316926818866485, 0.16105642272955495, 0.13421368560796246, 0.08052821136477747, 0.13421368560796246, 0.24039774605730008, 0.24039774605730008, 0.08013258201910002, 0.24039774605730008, 0.16026516403820004, 0.41169018423214065, 0.03430751535267838, 0.44599769958481905, 0.06861503070535677, 0.18620665642239503, 0.18620665642239503, 0.2793099846335925, 0.2793099846335925, 0.04512590038315428, 0.18050360153261713, 0.31588130268208, 0.2256295019157714, 0.09025180076630857, 0.13537770114946285, 0.18957845294800735, 0.11374707176880441, 0.18957845294800735, 0.07583138117920293, 0.07583138117920293, 0.18957845294800735, 0.07583138117920293, 0.07583138117920293, 0.17336431404686792, 0.08668215702343396, 0.17336431404686792, 0.17336431404686792, 0.34672862809373584, 0.4185015818870529, 0.3661888841511713, 0.10462539547176322, 0.05231269773588161, 0.24879224722392485, 0.24879224722392485, 0.16586149814928322, 0.16586149814928322, 0.08293074907464161, 0.2662570758062659, 0.325425314874325, 0.11833647813611818, 0.11833647813611818, 0.08875235860208863, 0.08875235860208863, 0.421591782125817, 0.11242447523355119, 0.22484895046710238, 0.056212237616775594, 0.028106118808387797, 0.056212237616775594, 0.056212237616775594, 0.028106118808387797, 0.04905926485036487, 0.14717779455109462, 0.39247411880291894, 0.19623705940145947, 0.14717779455109462, 0.05960983988727461, 0.685513158703658, 0.11921967977454923, 0.05960983988727461, 0.05960983988727461, 0.034003972350547515, 0.10201191705164254, 0.6800794470109504, 0.1700198617527376, 0.1641126504134405, 0.20514081301680062, 0.24616897562016074, 0.08205632520672025, 0.08205632520672025, 0.1641126504134405, 0.08205632520672025, 0.1414047712250997, 0.1414047712250997, 0.047134923741699905, 0.047134923741699905, 0.09426984748339981, 0.16497223309594966, 0.07070238561254985, 0.11783730935424976, 0.16497223309594966, 0.08615546539547864, 0.34462186158191455, 0.5169327923728718, 0.19686972855043036, 0.360927835675789, 0.06562324285014345, 0.03281162142507173, 0.1312464857002869, 0.06562324285014345, 0.09843486427521518, 0.14607025493825254, 0.23371240790120407, 0.17528430592590305, 0.17528430592590305, 0.17528430592590305, 0.02921405098765051, 0.02921405098765051, 0.38663564696696157, 0.19331782348348078, 0.28997673522522116, 0.3381388696633754, 0.21133679353960966, 0.04226735870792193, 0.1690694348316877, 0.04226735870792193, 0.08453471741584385, 0.08453471741584385, 0.399906384947679, 0.08569422534593121, 0.2570826760377936, 0.08569422534593121, 0.05712948356395414, 0.11425896712790828, 0.16428677742430678, 0.10952451828287119, 0.10952451828287119, 0.1916679069950246, 0.1916679069950246, 0.10952451828287119, 0.08214338871215339, 0.027381129570717797, 0.311838975830926, 0.10394632527697534, 0.05197316263848767, 0.155919487915463, 0.25986581319243834, 0.05197316263848767, 0.19652489613270904, 0.19652489613270904, 0.19652489613270904, 0.06550829871090301, 0.19652489613270904, 0.06550829871090301, 0.2909998509872961, 0.03233331677636623, 0.06466663355273246, 0.12933326710546492, 0.06466663355273246, 0.32333316776366233, 0.0969999503290987, 0.071000165974717, 0.4260009958483021, 0.4260009958483021], \"Term\": [\"able\", \"able\", \"able\", \"able\", \"able\", \"able\", \"accident\", \"accident\", \"accident\", \"accident\", \"accident\", \"accident\", \"accident\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"around\", \"around\", \"around\", \"around\", \"attack\", \"attack\", \"attack\", \"attack\", \"attack\", \"attack\", \"attack\", \"automate\", \"automate\", \"automate\", \"automate\", \"automate\", \"automate\", \"automate\", \"automate_vehicle\", \"automate_vehicle\", \"automate_vehicle\", \"automate_vehicle\", \"automate_vehicle\", \"automate_vehicle\", \"automate_vehicle\", \"automate_vehicle\", \"bad\", \"bad\", \"bad\", \"bad\", \"become\", \"become\", \"become\", \"become\", \"big\", \"big\", \"big\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"computer\", \"computer\", \"computer\", \"computer\", \"control\", \"control\", \"control\", \"control\", \"could\", \"could\", \"could\", \"could\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"death\", \"death\", \"death\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"driver\", \"driver\", \"driver\", \"driver\", \"driver\", \"driver\", \"driver\", \"driver\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"enough\", \"enough\", \"enough\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"every\", \"every\", \"every\", \"every\", \"every\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"fuck\", \"fuck\", \"fuck\", \"fuck\", \"fuck\", \"fuck\", \"future\", \"future\", \"future\", \"future\", \"future\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"hacker\", \"hacker\", \"hacker\", \"hand\", \"hand\", \"hand\", \"hand\", \"happen\", \"happen\", \"happen\", \"happen\", \"heart\", \"heart\", \"high\", \"high\", \"high\", \"high\", \"high\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"imagine\", \"imagine\", \"imagine\", \"issue\", \"issue\", \"issue\", \"keep\", \"keep\", \"keep\", \"keep\", \"kill\", \"kill\", \"kill\", \"kill\", \"kill\", \"kill\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"leave\", \"leave\", \"leave\", \"leave\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"life\", \"life\", \"life\", \"life\", \"life\", \"little\", \"little\", \"little\", \"little\", \"long\", \"long\", \"long\", \"long\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"many\", \"many\", \"many\", \"many\", \"mean\", \"mean\", \"mean\", \"mean\", \"might\", \"much\", \"much\", \"much\", \"much\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"never\", \"never\", \"never\", \"never\", \"never\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"point\", \"point\", \"point\", \"point\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"public\", \"public\", \"public\", \"public\", \"pull\", \"pull\", \"pull\", \"put\", \"put\", \"put\", \"put\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"right\", \"right\", \"right\", \"right\", \"road\", \"road\", \"road\", \"road\", \"road\", \"safety\", \"safety\", \"safety\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"secure\", \"secure\", \"secure\", \"secure\", \"secure\", \"secure\", \"security\", \"security\", \"security\", \"see\", \"see\", \"see\", \"see\", \"see\", \"since\", \"since\", \"since\", \"since\", \"since\", \"slow\", \"slow\", \"slow\", \"slow\", \"software\", \"software\", \"software\", \"software\", \"someone\", \"someone\", \"someone\", \"someone\", \"someone\", \"someone\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"start\", \"start\", \"start\", \"start\", \"start\", \"still\", \"still\", \"still\", \"still\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"tech\", \"tech\", \"tech\", \"tech\", \"tech\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"tesla\", \"tesla\", \"tesla\", \"tesla\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"though\", \"though\", \"though\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"vulnerable\", \"vulnerable\", \"vulnerable\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"yes\", \"yes\", \"yes\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 8, 10, 1, 6, 4, 7, 5, 3, 9]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el4952140535151431008407015677\", ldavis_el4952140535151431008407015677_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el4952140535151431008407015677\", ldavis_el4952140535151431008407015677_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el4952140535151431008407015677\", ldavis_el4952140535151431008407015677_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#vis = pyLDAvis.gensim_models.prepare(lda_topic_model, corpus, dictionary=lda_topic_model.id2word) \n",
    "\n",
    "pyLDAvis.save_html(vis, 'gensim_lda_result.html')\n",
    "\n",
    "# then display the HTML output using IPython.core.display:\n",
    "\n",
    "display(HTML('gensim_lda_result.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c2d7acc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nnum_topics=35\\n\\n# Print the 8 most important words for each topic\\nprint(\"Topics for reddit\")\\ncomponents_red,feat_names_red,lda_matrix_red = lda_pipeline_(df_sec_redt,num_topics)\\n# we will print only the 10 first\\nprint_top_words_for_topic(components_red[:10], feat_names_red , 10)\\n\\nnum_topics=20\\n\\n# Print the 8 most important words for each topic\\nprint(\"\\nTopics for twitter\")\\ncomponents_twi,feat_names_twi,lda_matrix_twi = lda_pipeline_(df_sec_twit,num_topics)\\nprint_top_words_for_topic(components_twi[:10], feat_names_twi, 10)\\n\\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# topics modeling\n",
    "\n",
    "'''\n",
    "\n",
    "num_topics=35\n",
    "\n",
    "# Print the 8 most important words for each topic\n",
    "print(\"Topics for reddit\")\n",
    "components_red,feat_names_red,lda_matrix_red = lda_pipeline_(df_sec_redt,num_topics)\n",
    "# we will print only the 10 first\n",
    "print_top_words_for_topic(components_red[:10], feat_names_red , 10)\n",
    "\n",
    "num_topics=20\n",
    "\n",
    "# Print the 8 most important words for each topic\n",
    "print(\"\\nTopics for twitter\")\n",
    "components_twi,feat_names_twi,lda_matrix_twi = lda_pipeline_(df_sec_twit,num_topics)\n",
    "print_top_words_for_topic(components_twi[:10], feat_names_twi, 10)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ff4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f055f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871aba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6efdade",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources:\n",
    "    https://towardsdatascience.com/unsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28\n",
    "        https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24#:~:text=Topic%20modeling%20is%20a%20type,document%20to%20a%20particular%20topic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
